# Information Retrieval / Retrieval Augmented Generation (RAG):

In many scenarios, we want our LLM to make use of knowledge it did not have access to while training. This could be in the form of information from a point in time after the model finish training or it could simply be information of a private dataset. In other cases, we might even use it as a means of reducing the risk of hallucination.

RAG is simply a combination of some kind of information retrieval component (called the retriever) alongside a text generation model. Given an input, the retriever finds a set of relevant documents from a given data source such as a vector database, or Wikipedia. This process is known as retrieval. To augment our generation, we concatenate the retrieved documents to the original prompt as additional context and feed this entire thing into a text generation model.

This is pretty useful in scenarios where knowledge can change. We do not need to do the expensive process of retraining a model on newer data. Rather, we can simply provide relevant information to it as and when needed. 

# Components:

In general, this system has two components. The first is the retriever which consists of some kind of storage, a search algorithm and sometimes other tools. For instance, it could be a relational database and a standard search algorithm. Or it could be a vectorstore with an embedding model. A vectorstore or a vector database is a data structure that supports the efficient storage of vector embeddings (which are generated by the embedding model). Here, we use a similarity metric such as cosine similarity for search. The second component is a text generation model such as GPT-4. In the following sections, we dive a little deeper into each of these sections.

## Storage Options

There's a lot of different knowledge bases that we can use as a data source. This could be existing databases you have access to, a collection of private data, an offline compilation of the entirety of Wikipedia, or even an API that performs Google searches! In many cases, you don't really need to do anything fancy. You just need some kind of retrieval algorithm (discussed in the next session) and you're good to go!

In other cases though, you might want something a little more advanced that can capture the semantics of your data. In such a case, using an embedding model and vector database might be the way to go. In this scenario, there are a lot of options you can use. For instance, LangChain supports almost [50 different options](https://python.langchain.com/docs/integrations/vectorstores/)!

## Retrieval Strategies

There are several different strategies that are employed to find relevant text. In general this means, given some kind of text input, we need to find the K most relevant entities from our storage and return them. K is a value set by the user to determine the maximum number of items to retrieve.

### Semantic Search

The most popular approach in the world of LLMs is semantic search (or sometimes referred to as a dense retrieval).

1. Create an embedding of the text input (using the embedding model).
2. Search the vector store for the K most similar embeddings (using the retriever).
3. Retrieve the text corresponding to the selected embeddings
4. Return

### Lexical Search

This refers to an older but still very relevant approach to search - the humble keyword search (such as [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) or [TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)). Also referred to as a sparse retrieval, this is especially useful when we can't use an embedding model or if the queries are simple enough to not need one. 

### Hybrid Search

This method combines several methods of retrieval together. As a simple example, we can use both semantic search and lexical search to rank items and then simple combine these rankings (known as reranking) by averaging them out or using something like [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf). We can think of this as running several algorithms to find a shortlist of potentially good items and then running a final step to filter the shortlist down to K options. This is useful in scenarios where any one approach does not retrieve everything we are looking for. For instance, we might want documents talking about a concept X but want to ensure that it uses a keyword Y.

### Embedding Models

An embedding model has a simple function. Take in some kind of input and output a vector of fixed size that encapsulates the meaning of the input. There's a bunch of different options you can use for this such as OpenAI's text-embedding models, sentence-transformers, LLaMa etc. LangChain supports [30+ options](https://python.langchain.com/docs/integrations/text_embedding/) including an integration with HuggingFace, which allows you to use practically any embedding model, such as the ever popular BERT model. There is also the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) which ranks different embedding models.

In general, we tend to use these models as-is for inference with no special fine-tuning applied to them. Thus the main concern is that of hosting such a model and the inference speed. This is because this model is used both during the data ingestion process and also during the retrieval process. Thus we need to find an option that gives us reasonable results while still being relatively cheap and fast.

## Text Generation Model:

Originally, the term RAG came from Meta's [RAG model](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/). Designed to be a seq2seq model trained simultaneously alongside an embedding model, today RAG is simply the term that refers to the information retrieval component of LLM systems. Nowadays, we just use an LLM instead.

In the case of most systems, you will already have an LLM of choice in mind. This will likely be something like GPT-3.5/4, Claude 2, PaLM 2, or LLaMa 2.

# Indexing Data / Data Ingestion

The first step towards an information retrieval system is to ingest all the external data we want to provide to our model. While the specific manner may vary between different providers, in general, this takes place over a few steps:

1. Load data in text format.
2. Chunk text. 
3. Process text for storage.
4. Store. 

While the above process is generic, when it comes to LLMs, the focus is usually on embeddings and vectorstores. Specifically, in step 3, we use a model to embed the chunks. In step 4, we store these embeddings in a vectorstore. The following section on chunks and chunk sizes is written from the perspective of using this concept of creating and storing embeddings.

Important Note: You do NOT need to use vectorstores for everything! Sometimes an existing relational database is more than enough to solve your problem.

### Determining Chunk Size

The are a lot of factors that can help determine what this chunk size should be:

**LLM Context Size**:

We want our chunks to be small enough that we can fit in one or more of them alongside the prompt as context. Thus the context size of the LLM being used plays an important role here.

**Expected Queries**:

We should take into account the kind of queries we expect. If the queries are going to be focused on high level information from different parts of the data, a larger chunk size could more easily capture this information. In such a scenario we can chunk paragraphs of documents or even entire documents. On the other hand, if we want to find very specific information, it might make more sense to use smaller chunks such as sentence-level chunking. 

In general, larger chunk sizes have trouble with specific questions while smaller chunks have trouble with more broad questions. This can of course be alleviated through some strategies such as hybrid search, but we'll cover that later.

**Embedding Model**:

We need to also consider the embedding model we use. Some models work well on individual sentences (such as sentence transformers) while other models are designed to work with chunks of specific sizes (such as OpenAI's text-embedding models).

Note: At the end of the day though, this is still a relatively new area that is still being actively explored. The solution might just be as simple as testing out a couple of different chunk sizes and seeing what works best for you!

### Chunking Strategies

Once we have a rough idea of this, we can decide a chunking strategy such as:

* **Fized-size chunking**: This is the simplest and least expensive method of chunking. We just divide the text into chunks of N-tokens. Just directly chunking them may however result in chunks losing some context. So in practice, there's usually a small amount of overlap between chunks. This is the approach generally used for OpenAI's text-embedding models.

* **Content-aware chunking**: In this method we usually chunk based on some idea of what we're chunking. This could be things like sentence-level or paragraph-level chunking.

* **Structure-aware chunking**: Sometimes we work with data that has some kind of known structure - such as Python code or Markdown. We can use this information while chunking. LangChain has an example [here](https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter).

* **Context-aware chunking**: In this scenario we use one of the chunking strategies from above, but in addition, we also store metadata. For instance, if we have were to chunk all of Wikipedia, we might store each article as a chunk and hold metadata on each article. We can then use this in a form of hybrid search during the retrieval phase.

Note: this is not a comprehensive view of all chunking methods. Rather, it's an explanation of commonly used chunking strategies. You can check out the [LangChain documentation](https://js.langchain.com/docs/modules/data_connection/document_transformers/) for more information on other chunking strategies.

# Querying Data / Retrieval:

The second phase is retrieval. This is performed during inference whenever a user asks a query.

1. Get a question from the user. 
    1.1. If this is a chat interface that may require prior context, concatenate the history with teh question and create a new prompt. This could be as simple as asking an LLM to create a one or two line question.
2. Use a retrieval strategy to find relevant text. 
3. Append the retrieved text to the question as context.
4. Pass this entire text into the LLM.
5. Return the output back to the user.

## Compression

Sometimes we might want to retrieve a lot more documents than the model's context size supports. In such scenarios, one option is to compress the retrieved documents. In its simplest form, this could be asking the model to summarize all the different documents. Then this summary can be passed as context instead of a concatenation of every retrieved document.

# Common Issues

## Semantic/Lexical Search isn't always enough

Sometimes semantic search doesn't always give us the best result since it works in an embedding space which may not necessarily capture everything we want. For example, if part of the query involves specific things like names or IDs, a keyword search is going to be much more useful than semantic search. On the other hand, a keyword search only takes the words themselves and not the semantic information of the query.

**Solution**: We can use hybrid search methods that combine different information. For instance we could perform a hybrid search that combines a vector search and BM25. We could also make use of other information such as graph structures, metadata etc.

## Complex Questions

What if we ask two or more questions at the same time? 
Example: Who is X and what did he do?

What if there are multiple parts to a single question?
Example: In what version of Product_ABC is Feature_XYZ available?

In both of these cases, the information we want might be split across several different documents. Thus we need the IR component to retrieve all these relevant documents (even if some of them don't contain parts of the prompt). 

**Solution**: One option in such a scenario is to simplify the prompts. This could involve us enforcing such a restriction, or we could use an LLM to break it down into individual questions. In essence we try to break the question down into its component parts, retrieve the relevant documents for each component, answer them separately and then combine the two answers. This is similar in nature to the [Plan-and-Execute](https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute) system popularized by [BabyAGI](https://github.com/yoheinakajima/babyagi).

## Too many relevant documents

If the query is a broad term or just a very popular term, then we might end up gettings 10s or even 100s of relevant documents. This is problematic since we run into the old issue of context size again. How do we compare or rank a large number of such documents? How do we determine the maximum number of documents to retrieve? 

**Solution**: The obvious answer is to just be more specific. Other options include using BM25 or a hybrid search. We could also use other information such as document metadata. In a worst case scenario, we could try and summarize everything using an LLM.

## Too many similar documents

Sometimes we might have a lot of very similar documents in our data. However, if all of them provide the same information for a given query, then they aren't as useful as taking one such document and getting other relevant documents. 

**Solution**: In such a scenario it might be helpful to retrieve a large number of documents and perform a filtering step after that. This could be as simple as enforcing a minimum distance between retrieved embeddings, or a loop of randomly picking an embedding, excluding all its immediate neighbors and then picking another embedding, or even an entire clustering algorithm where we pick one document from each cluster. 

## Too few documents

In other cases, we might just not find enough relevant documents. That is, a scenario where we want all relevant documents, even if it means we retrieve some junk. In such a case where we need high recall over everything else, what can we do?

**Solution**: We broaden our search. We can use hybrid or metadata-based searches, or we can use our LLM to create variations of our question.

## Mismatch between Retrieval Chunks & Generation Chunks

Smaller chunks do help improve the search experience but do not necessarily help us with the actual generation part. That is, while generating we might want to have more context than, say, a single sentence. 

**Solution**: Quite simply, we just decouple the two. There's a couple of things we can do to solve this. First is we could tie each embedding to a larger context around the the chunk being embedded. So if we were to chunk individual sentences, we would map each embedding to a paragraph containing the sentence but also the two sentences before and after the target sentence. This gives us a larger context that may help the model to generate better answers. Another alternative is to store a summary of the entire document or a large chunk instead.

# Resources

Here we list resources that were useful in the creation of this document.

* [LangChain Documentation](https://python.langchain.com/docs/get_started)
* [Beware Tunnel Vision in AI Retrieval](https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval)
* [Pinecone: Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)