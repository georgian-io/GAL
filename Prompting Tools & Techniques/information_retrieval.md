# Information Retrieval / Retrieval Augmented Generation (RAG):

## Table of Contents
- [Information Retrieval / Retrieval Augmented Generation (RAG):](#information-retrieval--retrieval-augmented-generation-rag)
  - [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [1. Components:](#1-components)
  - [1.1. Storage Options](#11-storage-options)
  - [1.2. Retrieval Strategies](#12-retrieval-strategies)
    - [1.2.1. Semantic Search](#121-semantic-search)
    - [1.2.2. Lexical Search](#122-lexical-search)
    - [1.2.3. Hybrid Search](#123-hybrid-search)
    - [1.2.4. Embedding Models](#124-embedding-models)
  - [1.3. Text Generation Model:](#13-text-generation-model)
  - [1.4. RAG in Action](#14-rag-in-action)
  - [1.5. RAG vs Fine-Tuning](#15-rag-vs-fine-tuning)
- [2. Indexing Data / Data Ingestion](#2-indexing-data--data-ingestion)
  - [2.1. Determining Chunk Size](#21-determining-chunk-size)
  - [2.2. Chunking Strategies](#22-chunking-strategies)
- [3. Querying Data / Retrieval:](#3-querying-data--retrieval)
  - [3.1. Compression](#31-compression)
- [4. Common Issues](#4-common-issues)
  - [4.1. Semantic/Lexical Search isn't always enough](#41-semanticlexical-search-isnt-always-enough)
  - [4.2. Complex Questions](#42-complex-questions)
  - [4.3. Too many relevant documents](#43-too-many-relevant-documents)
  - [4.4. Too many similar documents](#44-too-many-similar-documents)
  - [4.5. Too few documents](#45-too-few-documents)
  - [4.6. Mismatch between chunks for retrieval \& generation](#46-mismatch-between-chunks-for-retrieval--generation)
- [5. Resources](#5-resources)


# Introduction

In some scenarios, we may want our LLM to make use of knowledge it did not have access to while training. This knowledge could be in the form of information from a point in time after the model finished training or it could be information from a private dataset. In other cases, we may use the knowledge as a means of reducing the risk of hallucination.

Retrieval Augmented Generation (RAG) is a combination of some kind of information retrieval component (called the retriever) alongside a text generation model. Given an input, the retriever finds a set of relevant documents from a given data source such as a vector database, or Wikipedia. This process is known as retrieval. To augment our generation, we concatenate the retrieved documents to the original prompt as additional context and feed the combination into a text generation model.

This may be useful in scenarios where knowledge can change. By using RAG, we do not need to do the expensive process of retraining a model on newer data. Rather, we can provide relevant information to it as and when needed. 

Note: This document is not intended to go over the vector database side of retrieval. Rather, it considers the design and problem-solving aspects of this area from a prompting perspective. 
# 1. Components:

In general, this system has two components. The first is the retriever which consists of some kind of storage, a search algorithm and sometimes other tools. For instance, it could be a relational database and a standard search algorithm. Or it could be a vectorstore with an embedding model. A vectorstore or a vector database is a data structure that supports the efficient storage of vector embeddings (which are generated by the embedding model). Here, we use a similarity metric such as cosine similarity for search. The second component is a text generation model such as GPT-4. In the following sections, we dive a little deeper into each of these sections.

[[Back to top]](#)

## 1.1. Storage Options

There are different knowledge bases that we can use as a data source. Data sources may include existing databases you have access to, a collection of private data, an offline compilation of the entirety of Wikipedia or even an API that performs Google searches. To get started in this scenario, you only need a retrieval algorithm (discussed in the next session).

In other cases though, you might want something a little more advanced that can capture the semantics of your data. In such a case, using an embedding model and vector database might be the way to go. In this scenario, there are a lot of options you can use. For instance, LangChain supports almost [50 different options](https://python.langchain.com/docs/integrations/vectorstores/)!

[[Back to top]](#)

## 1.2. Retrieval Strategies

There are several different strategies that are employed to find relevant text. In general this means, given some kind of text input, we need to find the K most relevant entities from our storage and return them. K is a value set by the user to determine the maximum number of items to retrieve.

[[Back to top]](#)

### 1.2.1. Semantic Search

A popular approach in the world of LLMs is semantic search (or sometimes referred to as a dense retrieval).

1. Create an embedding of the text input (using the embedding model).
2. Search the vector store for the K most similar embeddings (using the retriever).
3. Retrieve the text corresponding to the selected embeddings.
4. Return.

[[Back to top]](#)

### 1.2.2. Lexical Search

This refers to an older but still relevant approach to search - the humble keyword search (such as [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) or [TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)). Also referred to as a sparse retrieval, lexical search can be  useful when we can't use an embedding model, or if the queries are simple enough to not need one. 

[[Back to top]](#)

### 1.2.3. Hybrid Search

The Hybrid Search method combines several methods of retrieval together. As a simple example, we can use both semantic search and lexical search to rank items and then simple combine these rankings (known as reranking) by averaging them out or using something like [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf). We can think of this as running several algorithms to find a shortlist of potentially good items and then running a final step to filter the shortlist down to K options. This is useful in scenarios where any one approach does not retrieve everything we are looking for. For instance, we may want to conduct a search to find documents talking about a concept X but at the same time  ensure that it uses a keyword Y.

[[Back to top]](#)

### 1.2.4. Embedding Models

An embedding model has a simple function. Take in some kind of input and output a vector of fixed size that encapsulates the meaning of the input. There are different options you can use for this such as OpenAI's text-embedding models, sentence-transformers, LLaMa etc. LangChain supports [30+ options](https://python.langchain.com/docs/integrations/text_embedding/) including an integration with HuggingFace, which allows you to use most embedding models, such as BERT. There is also the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) which ranks different embedding models.

In general, we tend to use these models as-is for inference with no special fine-tuning applied to them. Thus the main concern is that of hosting such a model and the inference speed. This is because this model is used both during the data ingestion process and also during the retrieval process. Thus we need to find an option that gives us reasonable results while still being relatively inexpensive and fast.

[[Back to top]](#)

## 1.3. Text Generation Model:

Originally, the term RAG came from Meta's [RAG model](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/). Designed to be a seq2seq model trained simultaneously alongside an embedding model, today RAG is simply the term that refers to the information retrieval component of LLM systems. Nowadays, we just use an LLM instead.

In the case of most systems, you will already have an LLM of choice in mind. This will likely be something like GPT-3.5/4, Claude 2, PaLM 2, or LLaMa 2.

[[Back to top]](#)

## 1.4. RAG in Action

The image below demonstrates a  typical RAG setup using vector databases.

![RAG System](images/rag_system.jpeg)

Source: [AWS: Retrieval Augmented Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)

The system receives a prompt and a user query. The user query is sent to the retrieval component which retrieves relevant content. The relevant content is appended to the prompt and query and then sent to the LLM to obtain the final answer.

[[Back to top]](#)

## 1.5. RAG vs Fine-Tuning

There has been some confusion on when to use RAG versus something like fine-tuning, especially in the context of allowing LLMs to make use of private data. Before we distinguish the two, let us first define fine-tuning. Fine-tuning is a technique used to train machine learning models on tasks in order to improve their performance on those tasks. At a high level, it can be thought of as a way to let the model learn new patterns and relationships in the type of data it is being fine-tuned on. 

Generally, fine-tuning is a good approach to use when either the domain of the data or the task in question is new to the model. In these scenarios, fine-tuning can typically be used to improve model performance. For instance, if you want to improve the performance of tasks like classification, summarization or question-answering, you may consider fine-tuning. Although it can be considered expensive to run due to the requirement for GPUs, techniques like [PeFT](https://huggingface.co/blog/peft) make it a little less resource-intensive. In addition, fine-tuning requires labeled data to learn from and in many cases requires access to the model itself, something only open source models offer (though there are exceptions like [GPT-3.5 fine-tuning](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)).

On the other hand, RAG can be useful when the structure or the language used in the data is the same but the content itself may be new to the model. In these cases where it's just a matter of providing the model with the knowledge required to complete a task, RAG may be the way to go. RAG is relatively more cost-effective than fine-tuning with extra charges being incurred only for hosting a database. In addition, it can be performed regardless of access to the model and does not require labeled data to train on. RAG can also reduce the risk of hallucination as the LLM can "cite" the information it uses. In addition, RAG is date-invariant. As long as the data store we have is up-to-date, the LLM used does not have to be. 

[[Back to top]](#)

# 2. Indexing Data / Data Ingestion

The first step towards an information retrieval system is to ingest all the external data we want to provide to our model. While the specific manner may vary between different providers, in general, this takes place over a few steps:

1. Load data in text format.
2. Chunk text. 
3. Process text for storage.
4. Store. 

While the above process is generic, when it comes to LLMs, the focus is usually on embeddings and vectorstores. Specifically, in step 3, we use a model to embed the chunks. In step 4, we store these embeddings in a vectorstore. The following section on chunks and chunk sizes is written from the perspective of using this concept of creating and storing embeddings.

Note: Vectorstores are not always needed to store everything. Sometimes an existing relational database can solve theproblem.

[[Back to top]](#)

## 2.1. Determining Chunk Size

The are other factors that can help determine what the chunk size should be:

**LLM Context Size**: We want our chunks to be small enough that we can fit in one or more of them alongside the prompt as context. Thus the context size of the LLM being used plays an important role here.

**Expected Queries**: We should take into account the kind of queries we expect. If the queries are going to be focused on high level information from different parts of the data, a larger chunk size could more easily capture this information. In such a scenario we can chunk paragraphs of documents or even entire documents. On the other hand, if we want to find very specific information, it might make more sense to use smaller chunks such as sentence-level chunking. In general, larger chunk sizes have trouble with specific questions while smaller chunks have trouble with more broad questions. This can of course be alleviated through some strategies such as hybrid search, but we'll cover that later.

**Embedding Model**: We need to also consider the embedding model we use. Some models work well on individual sentences (such as sentence transformers) while other models are designed to work with chunks of specific sizes (such as OpenAI's text-embedding models).

Note: Chunking is still a relatively new technique. The solution might be as simple as testing out a couple of different chunk sizes and seeing what works best for you.

[[Back to top]](#)

## 2.2. Chunking Strategies

Once we have a rough idea of the optimal chunk size, we can decide a chunking strategy such as:

* **Fized-size chunking**: This is a simple and inexpensive method of chunking. We just divide the text into chunks of N-tokens. Simply chunking them may however result in chunks losing some context. So in practice, there's usually a small amount of overlap between chunks. This is the approach generally used for OpenAI's text-embedding models.

* **Content-aware chunking**: In this method we chunk based on some idea of what we're chunking. This could be things like sentence-level or paragraph-level chunking.

* **Structure-aware chunking**: Sometimes we work with data that has some kind of known structure - such as Python code or Markdown. We can use this information while chunking. LangChain has an example [here](https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter).

* **Context-aware chunking**: In this scenario we use one of the chunking strategies from above, but in addition, we also store metadata. For instance, if we have were to chunk all of Wikipedia, we might store each article as a chunk and hold metadata on each article. We can then use this in a form of hybrid search during the retrieval phase.

Note: The techniques above are not a comprehensive view of all chunking methods. You can check out the [LangChain documentation](https://js.langchain.com/docs/modules/data_connection/document_transformers/) for more information on other chunking strategies.

[[Back to top]](#)

# 3. Querying Data / Retrieval:

The second phase is retrieval. This is performed during inference whenever a user asks a query.

1. Get a question from the user. 
2. Use a retrieval strategy to find relevant text. 
3. Append the retrieved text to the question as context.
4. Pass this entire text into the LLM.
5. Return the output back to the user.

[[Back to top]](#)

## 3.1. Compression

Sometimes our query might be quite big or might require some amount of the chat history. However a large query has the drawback of reducing the amount of information we can retrieve as context. As a solution to this, we could take the entire prompt alongside the chat history and create a summary of it using an LLM. In essence, we compress the large chat history and prompt combination to a simple one or two line prompt.

On a similar note, we might also want to retrieve more information than the model's supported context size. In this scenario too, we can use this concept of compression. This involves asking the model to summarize all the retrieved documents. Then this summary can be passed as context instead of a concatenation of the retrieved documents.

[[Back to top]](#)

# 4. Common Issues

Now we examine a number of common issues that arise when setting up an information retrieval system alongside an LLM. While we offer a number of solutions to each problem, the recent emergence of this area means that new solutions are still evolving and are subject to change.

## 4.1. Semantic/Lexical Search isn't always enough

Sometimes semantic search doesn't always give us an optimal result, since it works in an embedding space which may not necessarily capture everything we want. For example, if part of the query involves specific things like names or IDs, a keyword search is going to be more useful than semantic search. On the other hand, a keyword search only takes the words themselves and not the semantic information of the query.

**Solution**: We can use hybrid search methods that combine different information. For instance, we could perform a hybrid search that combines a vector search and BM25. We could also make use of other information such as graph structures, metadata etc.

[[Back to top]](#)

## 4.2. Complex Questions

What if we ask two or more questions at the same time? 
Example: Who is X and what did he do?

What if there are multiple parts to a single question?
Example: In what version of Product_ABC is Feature_XYZ available?

In both of these cases, the information we want might be split across several different documents. Thus we need the IR component to retrieve all these relevant documents (even if some of them don't contain parts of the prompt). 

**Solution**: One option in such a scenario is to simplify the prompts. We could do this manually, or we could use an LLM to break it down into individual questions. In essence, we try to break the question down into its component parts, retrieve the relevant documents for each component, answer them separately and then combine the two answers. This is the same as Least-to-Most prompting that we cover in `reasoning.md`. When used in an agent, this is similar to the [Plan-and-Execute](https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute) system popularized by [BabyAGI](https://github.com/yoheinakajima/babyagi).

[[Back to top]](#)

## 4.3. Too many relevant documents

If the query is a broad term or popular term, then we might end up getting tens or even hundreds of relevant documents. The large number of documents is problematic since we run into the old issue of context size again. How do we compare or rank a large number of such documents? How do we determine the maximum number of documents to retrieve? 

**Solution**: The obvious answer is to just be more specific. Other options include using BM25 or a hybrid search. We could also use other information such as document metadata. We could even try to summarize everything using an LLM.

[[Back to top]](#)

## 4.4. Too many similar documents

Sometimes we might have a lot of very similar documents in our data. However, if all of them provide the same information for a given query, then they aren't as useful as taking one such document and getting other relevant documents. 

**Solution**: In such a scenario it might be helpful to retrieve a large number of documents and perform a filtering step after that. This filtering step could be as simple as enforcing a minimum distance between retrieved embeddings, or a loop of randomly picking an embedding, excluding all its immediate neighbors and then picking another embedding, or even an entire clustering algorithm where we pick one document from each cluster. 

[[Back to top]](#)

## 4.5. Too few documents

In other cases, we might not find enough relevant documents. That is, a scenario where we want all relevant documents, even if it means we retrieve some irrelevant or not useful documents. In such a case where we need high recall over everything else, what can we do?

**Solution**: We broaden our search. We can use hybrid or metadata-based searches, or we can use our LLM to create variations of our question.

[[Back to top]](#)

## 4.6. Mismatch between chunks for retrieval & generation

Smaller chunks do help improve the search experience but do not necessarily help us with the actual generation part. That is, while generating we might want to have more context than, say, a single sentence. 

**Solution**: We can approach this by decoupling the two. We could try tying each embedding to a larger context around the chunk being embedded. So if we were to chunk individual sentences, we could map each embedding to a paragraph containing the sentence but also the two sentences before and after the target sentence. This chunking strategy gives us a larger context that may help the model to generate better answers. Another alternative is to store a summary of the entire document or a larger chunk instead.

[[Back to top]](#)

# 5. Resources

Here we list resources that were useful in the creation of this document.

* [LangChain Documentation](https://python.langchain.com/docs/get_started)
* [Beware Tunnel Vision in AI Retrieval](https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval)
* [Pinecone: Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)

[[Back to top]](#)