# Transferred Learnings Workshop on LLM Evaluation & Deployment

Large Language Models (LLMs) are increasingly used in various applications, from natural language processing tasks to content generation. The rapid advancement in generative AI and the role of LLMs in this advancement highlight the importance of properly evaluating and productionizing LLMs in a scalable and efficient way. This Workshop on LLM Evaluation and Deployment is designed to help in navigating the latest trends and tools for evaluating and deploying LLMs. The workshop contains the following streams:

**Stream 1: LLM Evaluation**

We'll cover various aspects of LLM evaluation including:

* Traditional evaluation, human evaluation, evaluating LLMs
* Evaluation metrics and considerations
* Evaluation tool and platform comparison, including open-sourced alternatives
* Hands-on end-to-end examples

To get the most out of the Evaluation stream, please come prepared with a data set and a model to evaluate

**Stream 2: LLM Deployment**

We'll cover the steps to deploy and serve your proof-of-concept including:

* How to use open-source (Mistral, LLaMa etc.) vs. closed-source (OpenAI, Anthropic etc.) models
* The basics of serving models using tools and libraries like TGI, vLLM and Triton.
* Advanced use cases such as multiple LoRAs, asynchronous inference, multimodal models.
* Deep dive into examples and cost analysis

To participate in the Deployment stream, any trained or fine-tuned model should suffice. 

