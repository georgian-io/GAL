{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (Optional): Use Langfuse to track evaluation and trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 01-llm-app-setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your keys if you didn't put it in your \".env\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    " \n",
    "# # get keys for your project from https://cloud.langfuse.com\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langfuse import Langfuse\n",
    "import openai\n",
    " \n",
    "# init\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gen_dataset = pd.read_csv(\"generated_qa.csv\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RAG QA Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_dataset(name=dataset_name)\n",
    "\n",
    "# Upload to Langfuse\n",
    "for _, row in gen_dataset.iterrows():\n",
    "  langfuse.create_dataset_item(\n",
    "      dataset_name=dataset_name,\n",
    "      # any python object or value\n",
    "      input=row[\"question\"],\n",
    "      # any python object or value, optional\n",
    "      expected_output={\n",
    "        \"ground_truth\": row[\"ground_truth\"],\n",
    "        \"ground_truth_context\": row[\"ground_truth_context\"]\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup custom evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 03-metrics-definition.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_context_correctness(output, expected_output):\n",
    "    ground_truth_context = expected_output[\"ground_truth_context\"]\n",
    "    retrieved_contexts = output[\"context\"] or []\n",
    "    return context_correctness(ground_truth_context, retrieved_contexts)\n",
    "\n",
    "\n",
    "def lf_ground_truth_context_rank(output, expected_output):\n",
    "    ground_truth_context = expected_output[\"ground_truth_context\"]\n",
    "    retrieved_contexts = output[\"context\"] or []\n",
    "    return ground_truth_context_rank(ground_truth_context, retrieved_contexts)\n",
    "\n",
    "\n",
    "def lf_context_rougel_score(output, expected_output):\n",
    "    ground_truth_context = expected_output[\"ground_truth_context\"]\n",
    "    retrieved_contexts = output[\"context\"] or []\n",
    "    return context_rougel_score(ground_truth_context, retrieved_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    " \n",
    "def run_my_custom_llm_app(input):\n",
    "    generationStartTime = datetime.now()\n",
    "\n",
    "    out = rag_chain.invoke(input)\n",
    "    \n",
    "    langfuse_generation = langfuse.generation(\n",
    "        name=\"rag-chain-qa\",\n",
    "        input=input,\n",
    "        output=out,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        start_time=generationStartTime,\n",
    "        end_time=datetime.now()\n",
    "        )\n",
    "\n",
    "    return out, langfuse_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(dataset_name)\n",
    "\n",
    "for item in dataset.items:\n",
    "    completion, langfuse_generation = run_my_custom_llm_app(item.input)\n",
    "\n",
    "    item.link(langfuse_generation, \"Exp 1\")\n",
    "\n",
    "    langfuse_generation.score(\n",
    "        name=\"context_correctness\",\n",
    "        value=lf_context_correctness(completion, item.expected_output)\n",
    "        )\n",
    "    langfuse_generation.score(\n",
    "        name=\"context_rank\",\n",
    "        value=lf_ground_truth_context_rank(completion, item.expected_output)\n",
    "        )\n",
    "    langfuse_generation.score(\n",
    "        name=\"context_rougel_score\",\n",
    "        value=lf_context_rougel_score(completion, item.expected_output)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please go to https://cloud.langfuse.com/ to see the trace and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
