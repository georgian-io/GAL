{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the earlier notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 01-llm-app-setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the documents split created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a chain to ask LLM to create question and answer. \n",
    "We'll use GPT-4 here to ensure a good Q&A generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class QAExample(BaseModel):\n",
    "    question: str = Field(description=\"question relevant to the given input\")\n",
    "    answer: str = Field(description=\"answer to the question\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"question\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=QAExample)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Given the following text, generate a set of question and answer about an information contained in the text.\\n{format_instructions}\\nText:\\n```\\n{text}\\n```\\n\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "gpt4_llm = ChatOpenAI(model_name=\"gpt-4-turbo-preview\", temperature=0)\n",
    "\n",
    "gen_qa_chain = prompt | gpt4_llm | parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAExample(question='What is the core controller of the autonomous agents discussed in the text?', answer='LLM (large language model)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_qa_chain.invoke({\"text\": splits[0].page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This looks good, let's run them over each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_qa = []\n",
    "\n",
    "for split in splits:\n",
    "    gen_qa.append(gen_qa_chain.invoke({\"text\": split.page_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QAExample(question='What is the core controller of the autonomous agents discussed in the text?', answer='LLM (large language model)'),\n",
       " QAExample(question='What is considered as utilizing the short-term memory of the model?', answer='In-context learning, as seen in Prompt Engineering, utilizes the short-term memory of the model.'),\n",
       " QAExample(question='What is the standard prompting technique mentioned for enhancing model performance on complex tasks?', answer='Chain of thought (CoT; Wei et al. 2022)'),\n",
       " QAExample(question='What does the Tree of Thoughts (Yao et al. 2023) extend and what is its primary methodology?', answer='Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, decomposing the problem into multiple thought steps, and generating multiple thoughts per step to create a tree structure.'),\n",
       " QAExample(question='What is the LLM+P approach described by Liu et al. 2023?', answer='The LLM+P approach involves relying on an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_qa[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's also make some questions where the answer is not in any parts of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Given the following text, generate a question about information not contained in the text, with the answer confirming that the information is not included.\\n{format_instructions}\\nText:\\n```\\n{text}\\n```\\n\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Increase the temperature to get more diverse questions and answers.\n",
    "gpt4_llm = ChatOpenAI(model_name=\"gpt-4-turbo-preview\", temperature=0.7)\n",
    "\n",
    "gen_qa_chain = prompt | gpt4_llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to make sure the questions generated are varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question='What are the specific environmental impacts mentioned in relation to the use of LLM-powered autonomous agents?' answer='The text does not mention any specific environmental impacts related to the use of LLM-powered autonomous agents.'\n",
      "question='Does the text provide any specific examples of real-world applications where LLM-powered autonomous agents have been deployed?' answer='No, the text does not provide specific examples of real-world applications where LLM-powered autonomous agents have been deployed.'\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    print(gen_qa_chain.invoke({\"text\": docs[0].page_content}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, let's run for a few more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_qa_no_answer = []\n",
    "\n",
    "for i in range(10):\n",
    "    gen_qa_no_answer.append(gen_qa_chain.invoke({\"text\": docs[0].page_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QAExample(question='Does the text provide any comparison of LLM-powered autonomous agents with human intelligence in terms of adaptability and creativity?', answer='No, the text does not provide a comparison of LLM-powered autonomous agents with human intelligence in terms of adaptability and creativity.'),\n",
       " QAExample(question='Does the text provide specific examples of external APIs used by MRKL systems?', answer='No, the text does not provide specific examples of external APIs used by MRKL systems.'),\n",
       " QAExample(question='What is the name of the author who developed the Reflexion framework mentioned in the text?', answer='The text does not provide the name of the author who developed the Reflexion framework.'),\n",
       " QAExample(question='What is the name of the author who developed the Reflexion framework?', answer='The name of the author who developed the Reflexion framework is not included in the text.'),\n",
       " QAExample(question='What specific methods were used to evaluate the performance of ChemCrow against GPT-4 by human experts?', answer='The text does not specify the exact methods used by human experts to evaluate the performance of ChemCrow against GPT-4.'),\n",
       " QAExample(question='What are the specific challenges faced by LLM-powered autonomous agents in interpreting and executing user commands that involve complex mathematical calculations?', answer='The text does not provide details on the specific challenges faced by LLM-powered autonomous agents in interpreting and executing user commands that involve complex mathematical calculations.'),\n",
       " QAExample(question='Does the text provide specific examples of APIs used by MRKL systems for external knowledge sources?', answer='No, the text does not provide specific examples of APIs used by MRKL systems for external knowledge sources.'),\n",
       " QAExample(question='Does the text provide any specific examples of LLM-powered autonomous agents being used in educational settings, such as tutoring or personalized learning?', answer='No, the text does not provide any examples of LLM-powered autonomous agents being used in educational settings.'),\n",
       " QAExample(question='Does the text provide any information on the specific programming languages used to implement the autonomous agents?', answer='No, the text does not specify which programming languages are used to implement the autonomous agents.'),\n",
       " QAExample(question='Does the text specify the academic qualifications of Lilian Weng?', answer='No, the text does not specify the academic qualifications of Lilian Weng.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_qa_no_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put this in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ground_truth_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the core controller of the autonomous ...</td>\n",
       "      <td>LLM (large language model)</td>\n",
       "      <td>LLM Powered Autonomous Agents\\n    \\nDate: Jun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is considered as utilizing the short-term...</td>\n",
       "      <td>In-context learning, as seen in Prompt Enginee...</td>\n",
       "      <td>Memory\\n\\nShort-term memory: I would consider ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the standard prompting technique menti...</td>\n",
       "      <td>Chain of thought (CoT; Wei et al. 2022)</td>\n",
       "      <td>Fig. 1. Overview of a LLM-powered autonomous a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the Tree of Thoughts (Yao et al. 202...</td>\n",
       "      <td>Tree of Thoughts extends CoT by exploring mult...</td>\n",
       "      <td>Tree of Thoughts (Yao et al. 2023) extends CoT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the LLM+P approach described by Liu et...</td>\n",
       "      <td>The LLM+P approach involves relying on an exte...</td>\n",
       "      <td>Another quite distinct approach, LLM+P (Liu et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>What are the specific challenges faced by LLM-...</td>\n",
       "      <td>The text does not provide details on the speci...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Does the text provide specific examples of API...</td>\n",
       "      <td>No, the text does not provide specific example...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Does the text provide any specific examples of...</td>\n",
       "      <td>No, the text does not provide any examples of ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Does the text provide any information on the s...</td>\n",
       "      <td>No, the text does not specify which programmin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Does the text specify the academic qualificati...</td>\n",
       "      <td>No, the text does not specify the academic qua...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the core controller of the autonomous ...   \n",
       "1   What is considered as utilizing the short-term...   \n",
       "2   What is the standard prompting technique menti...   \n",
       "3   What does the Tree of Thoughts (Yao et al. 202...   \n",
       "4   What is the LLM+P approach described by Liu et...   \n",
       "..                                                ...   \n",
       "71  What are the specific challenges faced by LLM-...   \n",
       "72  Does the text provide specific examples of API...   \n",
       "73  Does the text provide any specific examples of...   \n",
       "74  Does the text provide any information on the s...   \n",
       "75  Does the text specify the academic qualificati...   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0                          LLM (large language model)   \n",
       "1   In-context learning, as seen in Prompt Enginee...   \n",
       "2             Chain of thought (CoT; Wei et al. 2022)   \n",
       "3   Tree of Thoughts extends CoT by exploring mult...   \n",
       "4   The LLM+P approach involves relying on an exte...   \n",
       "..                                                ...   \n",
       "71  The text does not provide details on the speci...   \n",
       "72  No, the text does not provide specific example...   \n",
       "73  No, the text does not provide any examples of ...   \n",
       "74  No, the text does not specify which programmin...   \n",
       "75  No, the text does not specify the academic qua...   \n",
       "\n",
       "                                 ground_truth_context  \n",
       "0   LLM Powered Autonomous Agents\\n    \\nDate: Jun...  \n",
       "1   Memory\\n\\nShort-term memory: I would consider ...  \n",
       "2   Fig. 1. Overview of a LLM-powered autonomous a...  \n",
       "3   Tree of Thoughts (Yao et al. 2023) extends CoT...  \n",
       "4   Another quite distinct approach, LLM+P (Liu et...  \n",
       "..                                                ...  \n",
       "71                                                     \n",
       "72                                                     \n",
       "73                                                     \n",
       "74                                                     \n",
       "75                                                     \n",
       "\n",
       "[76 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gen_qa_lst = []\n",
    "\n",
    "for i in range(len(gen_qa)):\n",
    "    qa_dict = gen_qa[i].dict()\n",
    "    qa_dict[\"ground_truth_context\"] = splits[i].page_content\n",
    "    gen_qa_lst.append(qa_dict)\n",
    "    \n",
    "for qa in gen_qa_no_answer:\n",
    "    qa_dict = qa.dict()\n",
    "    qa_dict[\"ground_truth_context\"] = \"\"\n",
    "    gen_qa_lst.append(qa_dict)\n",
    "\n",
    "gen_dataset = pd.DataFrame(gen_qa_lst)\n",
    "gen_dataset.rename(columns={\"answer\": \"ground_truth\"}, inplace=True)\n",
    "gen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dataset.to_csv(\"generated_qa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
