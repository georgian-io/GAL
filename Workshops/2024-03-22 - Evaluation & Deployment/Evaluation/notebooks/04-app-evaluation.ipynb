{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate the application\n",
    "\n",
    "## 1. Run the app on our evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 01-llm-app-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gen_dataset = pd.read_csv('generated_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dataset[\"answer\"] = None\n",
    "gen_dataset[\"contexts\"] = None\n",
    "\n",
    "for idx, item in gen_dataset.iloc[:2].iterrows():\n",
    "    result = rag_chain.invoke(item.question)\n",
    "    gen_dataset.at[idx, \"answer\"] = result[\"answer\"]\n",
    "    gen_dataset.at[idx, \"contexts\"] = result[\"context\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ground_truth_context</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the core controller of the autonomous ...</td>\n",
       "      <td>LLM (large language model)</td>\n",
       "      <td>LLM Powered Autonomous Agents\\n    \\nDate: Jun...</td>\n",
       "      <td>The core controller of the autonomous agents d...</td>\n",
       "      <td>[LLM Powered Autonomous Agents\\n    \\nDate: Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is considered as utilizing the short-term...</td>\n",
       "      <td>In-context learning, as seen in Prompt Enginee...</td>\n",
       "      <td>Memory\\n\\nShort-term memory: I would consider ...</td>\n",
       "      <td>Utilizing the short-term memory of the model i...</td>\n",
       "      <td>[Memory\\n\\nShort-term memory: I would consider...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the core controller of the autonomous ...   \n",
       "1  What is considered as utilizing the short-term...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                         LLM (large language model)   \n",
       "1  In-context learning, as seen in Prompt Enginee...   \n",
       "\n",
       "                                ground_truth_context  \\\n",
       "0  LLM Powered Autonomous Agents\\n    \\nDate: Jun...   \n",
       "1  Memory\\n\\nShort-term memory: I would consider ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The core controller of the autonomous agents d...   \n",
       "1  Utilizing the short-term memory of the model i...   \n",
       "\n",
       "                                            contexts  \n",
       "0  [LLM Powered Autonomous Agents\\n    \\nDate: Ju...  \n",
       "1  [Memory\\n\\nShort-term memory: I would consider...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_dataset.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run evaluation\n",
    "\n",
    "This might take a few minutes. We call our pre-defined evaluation functions and also run ragas on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 03-metrics-definition.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6630c853bf4b7989537fe841c85d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5460de2a7a4aadb77ec9d413f7f81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315be738b6ae41c993fcc53c3ce3bc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_lst = []\n",
    " \n",
    "for idx, row in gen_dataset.iloc[:2].iterrows(): # Subsetting to make it go faster\n",
    "    custom_eval_results = {\n",
    "        \"context_correctness\": context_correctness(row[\"ground_truth_context\"], row[\"contexts\"]),\n",
    "        \"ground_truth_context_rank\": ground_truth_context_rank(row[\"ground_truth_context\"], row[\"contexts\"]),\n",
    "        \"context_rougel_score\": context_rougel_score(row[\"ground_truth_context\"], row[\"contexts\"]),\n",
    "    }\n",
    "\n",
    "    ragas_eval_results = evaluate_w_ragas(row)\n",
    "    results_lst.append(custom_eval_results | ragas_eval_results)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_correctness</th>\n",
       "      <th>ground_truth_context_rank</th>\n",
       "      <th>context_rougel_score</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.589765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.469287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   context_correctness  ground_truth_context_rank  context_rougel_score  \\\n",
       "0                 True                          0                   1.0   \n",
       "1                 True                          0                   1.0   \n",
       "\n",
       "   context_precision  faithfulness  answer_correctness  \n",
       "0                1.0           1.0            0.589765  \n",
       "1                1.0           1.0            0.469287  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We can aggregate these metrics to get a single number for each of them and we have a good evaluation of our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
